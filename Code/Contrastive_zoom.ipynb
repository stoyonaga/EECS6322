{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "# Data Visualization\n",
    "import plotly.express as px\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# Experimental\n",
    "from torchvision import models\n",
    "from contrastive_learner import ContrastiveLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionNetwork, self).__init__()\n",
    "        # Input has images of dimension (100 x 100)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 100, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(100 * 25 * 25, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(32)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(64)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "\n",
    "        # Attention Network applied after the first convolutional layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.dropout(x)\n",
    "        \"\"\"\n",
    "        Goal: Apply attention after first convolution to get spatial features\n",
    "          i) Calculate the attention weights\n",
    "        \"\"\"\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.attention(x)\n",
    "        # Normalize attention weights to sum up to 1\n",
    "        attention_weights = F.normalize(attention_weights, p=1, dim=(2, 3))\n",
    "        # Incorporate attention into the forward pass\n",
    "        x = x * attention_weights\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_1 = 0.2\n",
    "colon_dataset =  datasets.ImageFolder(\n",
    "    '/content/drive/MyDrive/Graduate/Courses/Winter 2024/EECS 6322/Course Project/CustomData/Colon',\n",
    "    transform = transforms.Compose([\n",
    "        # Downscale the image by s_1\n",
    "        transforms.Resize(int(s_1 * 500)),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colon Dataset\n",
    "loaders = {\n",
    "    'train' : DataLoader(\n",
    "    colon_dataset,\n",
    "    batch_size=5,\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom-In with Contrastive Learning\n",
    "#While this works we didn't have\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    betas = (0.9, 0.999),\n",
    "    lr = 0.001,\n",
    "    weight_decay=1e-5\n",
    "    )\n",
    "for epoch in range(100):\n",
    "    learner.train()\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for image, label in tqdm(loaders['train']):\n",
    "      loss = learner(image)\n",
    "      optimizer.zero_grad()\n",
    "      if epoch % 10 == 0:\n",
    "        print(loss) #Printing the loss every 10 epochs\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      learner.update_moving_average()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
