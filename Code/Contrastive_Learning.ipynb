{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "# Data Visualization\n",
    "import plotly.express as px\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# Experimental\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrastive Learning for Zoom-In-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AttentionNetwork() #Our attention Network we already created\n",
    "\n",
    "#This is the feature network for which we will pass through \n",
    "class Feature_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Feature_Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 100, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(100 * 25 * 25, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "    def features_network_forward(zoom_in_model):  #The objective here is to make sure we can extract the features of the Zoom-In-Network\n",
    "        print('not done yet')\n",
    "\n",
    "\n",
    "\n",
    "class Contrastive_Learning(nn.mo):\n",
    "\n",
    "    def sample_attentions(alpha, beta, N): #Sampling for alpha and beta \n",
    "        #We want to get the tiles with the lowest attention weights, hence we use inversion at this point\n",
    "        \n",
    "        prob_distribution_1 = 1 - alpha\n",
    "        prob_distribution_2 = 1 - beta\n",
    "        for x in range (N):\n",
    "            tile_locations = np.random.choice(len(alpha), N, True, prob_distribution_1)\n",
    "            samples_subtiles = np.random.choice(len(beta), N, False, prob_distribution_2)\n",
    "\n",
    "        return (tile_locations, samples_subtiles)\n",
    "\n",
    "\n",
    "    Feature_Network.features_network_forward(sample_attentions(alpha, beta, N))\n",
    "\n",
    "    \n",
    "    def loss(p,t,psi, X): #This is the objective we want to \n",
    "\n",
    "        return -np.sum(np.log(1 - model(x,1)) for x in X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
